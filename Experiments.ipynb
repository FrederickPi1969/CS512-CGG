{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                              | 39/5000 [00:00<00:13, 365.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cpu\n",
      "\n",
      "============= Generating Data ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:12<00:00, 385.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from model import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from utils_for_experiments import *\n",
    "# from transform_wrappers_multiprocessing import *\n",
    "from transform_wrappers import *\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from discretize import *\n",
    "from visualization import *\n",
    "\n",
    "\n",
    "np.set_printoptions(linewidth=np.inf)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device = \"cpu\"\n",
    "    print(f\"using device {device}\")\n",
    "\n",
    "    ####################     Generation parameters     #######################################################\n",
    "    dataArgs = dict()\n",
    "\n",
    "    maximum_number_of_nodes_n = \"20\" #@param [12, 24, 30, 48]\n",
    "    dataArgs[\"max_n_node\"] = int(maximum_number_of_nodes_n)\n",
    "\n",
    "    range_of_linkage_probability_p = \"0.0, 1.0\" #@param [[0.0,1.0], [0.2,0.8], [0.5,0.5]]\n",
    "    dataArgs[\"p_range\"] = [float(range_of_linkage_probability_p.split(\",\")[0]), float(range_of_linkage_probability_p.split(\",\")[1])]\n",
    "\n",
    "    node_attributes = \"degree\" #@param [\"uniform\", \"degree\", \"random\"]\n",
    "    dataArgs[\"node_attr\"] = node_attributes\n",
    "\n",
    "    number_of_graph_instances = \"5000\" #@param [1, 100, 1000, 10000, 25000, 50000, 100000, 200000, 500000, 1000000]\n",
    "    dataArgs[\"n_graph\"] = int(number_of_graph_instances)\n",
    "\n",
    "    dataArgs[\"upper_triangular\"] = False\n",
    "    A, Attr, Param, Topol = generate_data_v2(dataArgs)\n",
    "    # g, a, attr = unpad_data(A[0], Attr[0])\n",
    "\n",
    "    ####################     Model parameters     #######################################################\n",
    "    modelArgs = {\"gnn_filters\": 2, \"conv_filters\": 16, \"kernel_size\": 3}\n",
    "\n",
    "    number_of_latent_variables= \"20\" #@param [1, 2, 3, 4, 5]\n",
    "    modelArgs[\"latent_dim\"] = int(number_of_latent_variables)\n",
    "\n",
    "    trainArgs = dict()\n",
    "\n",
    "    weight_graph_reconstruction_loss = \"30\" #@param [0, 1, 2, 3, 5, 10, 20]\n",
    "    weight_attribute_reconstruction_loss = \"5\" #@param [0, 1, 2, 3, 5, 10, 20]\n",
    "    beta_value = \"20\" #@param [0, 1, 2, 3, 5, 10, 20]\n",
    "    trainArgs[\"loss_weights\"] = [int(weight_graph_reconstruction_loss), int(weight_attribute_reconstruction_loss), int(beta_value)]\n",
    "\n",
    "    epochs = \"35\" #@param [10, 20, 50]\n",
    "    trainArgs[\"epochs\"] = int(epochs)\n",
    "    batch_size = \"512\" #@param [2, 4, 8, 16, 32, 128, 512, 1024]\n",
    "    trainArgs[\"batch_size\"] = int(batch_size)\n",
    "    early_stop = \"2\" #@param [1, 2, 3, 4, 10]\n",
    "    trainArgs[\"early_stop\"] = int(early_stop)\n",
    "    train_test_split = \"0.2\" #@param [0.1, 0.2, 0.3, 0.5]\n",
    "    train_validation_split = \"0.1\" #@param [0.1, 0.2, 0.3, 0.5]\n",
    "    trainArgs[\"data_split\"] = float(train_test_split)\n",
    "    trainArgs[\"validation_split\"] = float(train_validation_split)\n",
    "    lr = \"0.001\"  #@param [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "    trainArgs[\"lr\"] = float(lr)\n",
    "\n",
    "\n",
    "\n",
    "    ## Train and Test Split _______________________________________________\n",
    "\n",
    "    A_train = torch.from_numpy(A[:int((1-trainArgs[\"data_split\"]-trainArgs[\"validation_split\"])*A.shape[0])])\n",
    "    Attr_train = generate_batch(torch.from_numpy(Attr[:int((1-trainArgs[\"data_split\"]-trainArgs[\"validation_split\"])*Attr.shape[0])]), trainArgs[\"batch_size\"])\n",
    "    Param_train = generate_batch(torch.from_numpy(Param[:int((1-trainArgs[\"data_split\"]-trainArgs[\"validation_split\"])*Param.shape[0])]), trainArgs[\"batch_size\"])\n",
    "    Topol_train = generate_batch(torch.from_numpy(Topol[:int((1-trainArgs[\"data_split\"]-trainArgs[\"validation_split\"])*Topol.shape[0])]), trainArgs[\"batch_size\"])\n",
    "\n",
    "    A_validate = torch.from_numpy(A[int((1-trainArgs[\"data_split\"]-trainArgs[\"validation_split\"])*A.shape[0]):int((1-trainArgs[\"data_split\"])*Attr.shape[0])])\n",
    "    Attr_validate = generate_batch(torch.from_numpy(Attr[int((1-trainArgs[\"data_split\"]-trainArgs[\"validation_split\"])*Attr.shape[0]):int((1-trainArgs[\"data_split\"])*Attr.shape[0])]), trainArgs[\"batch_size\"])\n",
    "    Param_validate = generate_batch(torch.from_numpy(Param[int((1-trainArgs[\"data_split\"]-trainArgs[\"validation_split\"])*Param.shape[0]):int((1-trainArgs[\"data_split\"])*Attr.shape[0])]), trainArgs[\"batch_size\"])\n",
    "    Topol_validate = generate_batch(torch.from_numpy(Topol[int((1-trainArgs[\"data_split\"]-trainArgs[\"validation_split\"])*Topol.shape[0]):int((1-trainArgs[\"data_split\"])*Attr.shape[0])]), trainArgs[\"batch_size\"])\n",
    "\n",
    "    A_test = torch.from_numpy(A[int((1-trainArgs[\"data_split\"])*A.shape[0]):])\n",
    "    Attr_test = generate_batch(torch.from_numpy(Attr[int((1-trainArgs[\"data_split\"])*Attr.shape[0]):]), trainArgs[\"batch_size\"])\n",
    "    Param_test = generate_batch(torch.from_numpy(Param[int((1-trainArgs[\"data_split\"])*Param.shape[0]):]), trainArgs[\"batch_size\"])\n",
    "    Topol_test = generate_batch(torch.from_numpy(Topol[int((1-trainArgs[\"data_split\"])*Topol.shape[0]):]), trainArgs[\"batch_size\"])\n",
    "\n",
    "\n",
    "    # print(A_train.shape)\n",
    "    # print(len(Attr_train), Attr_train[0].shape)\n",
    "\n",
    "    ## build graph_conv_filters\n",
    "    SYM_NORM = True\n",
    "    A_train_mod = generate_batch(preprocess_adj_tensor_with_identity(torch.squeeze(A_train, -1), SYM_NORM), trainArgs[\"batch_size\"])\n",
    "    A_validate_mod = generate_batch(preprocess_adj_tensor_with_identity(torch.squeeze(A_validate, -1), SYM_NORM), trainArgs[\"batch_size\"])\n",
    "    A_test_mod = generate_batch(preprocess_adj_tensor_with_identity(torch.squeeze(A_test, -1), SYM_NORM), trainArgs[\"batch_size\"])\n",
    "\n",
    "    A_train = generate_batch(A_train, trainArgs[\"batch_size\"])\n",
    "    A_validate = generate_batch(A_validate, trainArgs[\"batch_size\"])\n",
    "    A_test = generate_batch(A_test, trainArgs[\"batch_size\"])\n",
    "\n",
    "    train_data = (Attr_train, A_train_mod, Param_train, Topol_train)\n",
    "    validate_data = (Attr_validate, A_validate_mod, Param_validate, Topol_validate)\n",
    "    test_data = (Attr_test, A_test_mod, Param_test, Topol_test)\n",
    "\n",
    "    # attribute first -> (n, n), adjacency second -> (n, n, 1)\n",
    "    modelArgs[\"input_shape\"], modelArgs[\"output_shape\"] = ((Attr_train[0].shape[1], Attr_train[0].shape[2]), (int(A_train_mod[0].shape[1] / modelArgs[\"gnn_filters\"]), A_train_mod[0].shape[2], 1)),\\\n",
    "                                                          ((Attr_test[0].shape[1], Attr_test[0].shape[1]), (int(A_test_mod[0].shape[1] / modelArgs[\"gnn_filters\"]), A_test_mod[0].shape[2], 1))\n",
    "    # print(modelArgs[\"input_shape\"], modelArgs[\"output_shape\"])\n",
    "    # print(A_train[0].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " =================Extracting useful information=====================\n",
      "Model loss 2380.786865234375 \n",
      "VAE performance:  \n",
      " Accuracy: 0.9141899999999935,  \n",
      " Precision: 0.7439750526004668,  \n",
      " Recall: 0.8276554645262381,  \n",
      " F1 Score: 0.7835875048821409\n",
      "\n",
      "--- Truth topology (averaged) ---\n",
      " density: 0.24532481203007417 \n",
      " diameter: -1.0 \n",
      " clustering coefficient: 0.27123789549619354 \n",
      " edges: 46.611714285714285 \n",
      " avgerage degree 0.24532481203007417\n",
      "\n",
      "--- Reconstructed topology (averaged) ---\n",
      " density: 0.2893398496240597 \n",
      " diameter: -1.0 \n",
      " clustering coefficient: 0.40446849661076467 \n",
      " edges: 54.97457142857143 \n",
      " avgerage degree 0.2893398496240597\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "######################      VAE        ##########################################\n",
    "operation_name = \"density\"  ## [\"transitivity\", \"density\", \"forest fire ...\"]\n",
    "# param_path = operation_name + \"_pretrained\" + \"_\" + maximum_number_of_nodes_n\n",
    "param_path = \".\"\n",
    "vae = torch.load(param_path + \"/vae.model\")\n",
    "\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "batched_z = []\n",
    "batched_A_hat = []\n",
    "batched_Attr_hat = []\n",
    "batched_A_hat_discretized = []\n",
    "batched_A_hat_discretized_test = []\n",
    "batched_gcn_filters_from_A_hat = []\n",
    "batched_z_test = []\n",
    "batched_A_hat_test = []\n",
    "batched_Attr_hat_test = []\n",
    "batched_gcn_filters_from_A_hat_test = []\n",
    "batched_A_hat_raw_train = []\n",
    "batched_A_hat_raw_test = []\n",
    "batched_A_hat_max_train = []\n",
    "batched_A_hat_max_test = []\n",
    "batched_A_hat_min_train = []\n",
    "batched_A_hat_min_test = []\n",
    "print(\"\\n\\n =================Extracting useful information=====================\")\n",
    "vae.eval()\n",
    "\n",
    "def index_of(my_list, target):\n",
    "    try: return my_list.index(target)\n",
    "    except: return dataArgs[\"max_n_node\"]\n",
    "\n",
    "for e in range(1):\n",
    "    loss_cum = 0\n",
    "    for i in range(len(Attr_train)):\n",
    "        attr = Attr_train[i].float().to(device)\n",
    "        A = A_train[i].float().to(device)\n",
    "        graph_conv_filters = A_train_mod[i].float().to(device)\n",
    "\n",
    "        z, z_mean, z_log_var, A_hat, attr_hat, A_hat_raw, max_score_per_node, min_score_per_node = vae(attr, graph_conv_filters)\n",
    "\n",
    "        if e + 1 == 1:\n",
    "            batched_z.append(z.detach())\n",
    "            batched_Attr_hat.append(attr_hat.detach())\n",
    "            batched_A_hat.append(A_hat.detach())\n",
    "            temp = A_hat.detach().cpu()\n",
    "            batched_gcn_filters_from_A_hat.append(preprocess_adj_tensor_with_identity(torch.squeeze(temp, -1), symmetric = False))\n",
    "\n",
    "            A_discretize = A.cpu().squeeze().numpy()\n",
    "            A_hat_discretize = A_hat.detach().cpu().squeeze().numpy()\n",
    "            discretizer = Discretizer(A_discretize, A_hat_discretize)\n",
    "            A_hat_discretize = discretizer.discretize('hard_threshold')\n",
    "            A_hat_discretize = torch.unsqueeze(torch.from_numpy(A_hat_discretize), -1)\n",
    "\n",
    "            batched_A_hat_discretized.append(A_hat_discretize)\n",
    "            batched_A_hat_raw_train.append(A_hat_raw.detach())\n",
    "            batched_A_hat_max_train.append(max_score_per_node.detach())\n",
    "            batched_A_hat_min_train.append(min_score_per_node.detach())\n",
    "\n",
    "            # count = 0\n",
    "            for j in range(len(batched_A_hat_discretized[i])):\n",
    "                temp = list(torch.diag(batched_A_hat_discretized[i][j].detach().reshape(dataArgs[\"max_n_node\"], -1)))[::-1]\n",
    "                pred_node_num = dataArgs[\"max_n_node\"] - index_of(list(temp), 1)\n",
    "                Param_train[i][j][-1] = pred_node_num  # predicted node num have ~96% acc\n",
    "                true_node_num = int(Param_train[i][j][0])\n",
    "                # print(pred_node_num)\n",
    "                # print(true_node_num)\n",
    "\n",
    "                # count += pred_node_num == true_node_num\n",
    "            # print(f\"node prediction accuracy : {count / len(batched_A_hat_discretized[i])}\")\n",
    "\n",
    "        loss = loss_func((A, attr), (A_hat, attr_hat), z_mean, z_log_var, trainArgs, modelArgs)\n",
    "        loss_cum += loss.item()\n",
    "\n",
    "    print(\"Model loss {} \".format(loss_cum / len(Attr_train)))\n",
    "\n",
    "\n",
    "    loss_cum = 0\n",
    "    for i in range(len(Attr_validate)):\n",
    "        attr = Attr_validate[i].float().to(device)\n",
    "        A = A_validate[i].float().to(device)\n",
    "        graph_conv_filters = A_validate_mod[i].float().to(device)\n",
    "\n",
    "        z, z_mean, z_log_var, A_hat, attr_hat, A_hat_raw, max_score_per_node, min_score_per_node = vae(attr, graph_conv_filters)\n",
    "        loss = loss_func((A, attr), (A_hat, attr_hat), z_mean, z_log_var, trainArgs, modelArgs)\n",
    "        loss_cum += loss.item()\n",
    "\n",
    "        if e + 1 == 1:\n",
    "            batched_z_test.append(z.detach())\n",
    "            batched_Attr_hat_test.append(attr_hat.detach())\n",
    "            batched_A_hat_test.append(A_hat.detach())\n",
    "            temp = A_hat.detach().cpu()\n",
    "            batched_gcn_filters_from_A_hat_test.append(preprocess_adj_tensor_with_identity(torch.squeeze(temp, -1), symmetric = False))\n",
    "\n",
    "            A_discretize = A.cpu().squeeze().numpy()\n",
    "            A_hat_discretize = A_hat.detach().cpu().squeeze().numpy()\n",
    "            discretizer = Discretizer(A_discretize, A_hat_discretize)\n",
    "            A_hat_discretize = discretizer.discretize('hard_threshold', threshold=0.7)\n",
    "            A_hat_discretize = torch.unsqueeze(torch.from_numpy(A_hat_discretize), -1)\n",
    "\n",
    "            batched_A_hat_discretized_test.append(A_hat_discretize)\n",
    "            batched_A_hat_raw_test.append(A_hat_raw.detach())\n",
    "            batched_A_hat_max_test.append(max_score_per_node.detach())\n",
    "            batched_A_hat_min_test.append(min_score_per_node.detach())\n",
    "\n",
    "    # print(\"At Epoch {}, validation loss {} \".format(e + 1, loss_cum / len(Attr_validate)))\n",
    "\n",
    "a,p,r,f = compute_score_batched(A_train, batched_A_hat_discretized)\n",
    "print(f\"VAE performance:  \\n Accuracy: {a},  \\n Precision: {p},  \\n Recall: {r},  \\n F1 Score: {f}\\n\")\n",
    "\n",
    "density_ori, diameter_ori, cluster_coef_ori, edges_ori, avg_degree_ori = topological_measure(A_train)\n",
    "density_hat, diameter_hat, cluster_coef_hat, edges_hat, avg_degree_hat = topological_measure(batched_A_hat_discretized)\n",
    "print(f\"--- Truth topology (averaged) ---\\n density: {density_ori} \\n diameter: {diameter_ori} \"\n",
    "      f\"\\n clustering coefficient: {cluster_coef_ori} \\n edges: {edges_ori} \\n avgerage degree {avg_degree_ori}\\n\")\n",
    "print(f\"--- Reconstructed topology (averaged) ---\\n density: {density_hat} \\n diameter: {diameter_hat} \"\n",
    "      f\"\\n clustering coefficient: {cluster_coef_hat} \\n edges: {edges_hat} \\n avgerage degree {avg_degree_hat}\\n\")\n",
    "\n",
    "\n",
    "################ Load Discriminator #####################################\n",
    "discriminator = torch.load(param_path + \"/discriminator.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################# Steering GAN   ####################################\n",
    "\n",
    "## training tip: same batch, same alpha!\n",
    "\n",
    "# w = torch.randn_like(batched_z[0][0], requires_grad=True).unsqueeze(0).to(device)\n",
    "w = torch.load(param_path + \"/w_density.pt\")\n",
    "a_w1 = torch.load(param_path + \"/a_w1_density.pt\")\n",
    "a_w2 = torch.load(param_path + \"/a_w2_density.pt\")\n",
    "a_b1 = torch.load(param_path + \"/a_b1_density.pt\")\n",
    "a_b2 = torch.load(param_path + \"/a_b2_density.pt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Initialize generator\n",
    "generator = Decoder_v2(modelArgs, trainArgs, device).to(device)\n",
    "\n",
    "decoder_weight = dict(vae.decoder.named_parameters())\n",
    "generator_weight = dict(generator.named_parameters())\n",
    "for k in generator_weight.keys():\n",
    "    assert k in decoder_weight\n",
    "    generator_weight[k] = decoder_weight[k]\n",
    "generator.eval()\n",
    "\n",
    "\n",
    "discriminator.eval()\n",
    "## operation = \"transitivity\", \"density\", \"node_count\"\n",
    "\n",
    "# transform = GraphTransform(dataArgs[\"max_n_node\"], operation = operation_name, sigmoid = False)\n",
    "transform = GraphTransform(dataArgs[\"max_n_node\"], operation = operation_name, sigmoid = False)\n",
    "w_epochs = 1  ### adjust epoch here!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:04<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================== G(z + aw) v.s. edit(G(z))  results =============================\n",
      "accuracy: 0.8927507142857133\n",
      "precision: 0.8855145540822386\n",
      "recall: 0.6271413799285077\n",
      "f1 score: 0.7342619123192717\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_train = []\n",
    "w_A_train = []\n",
    "w_A_hat_train = []\n",
    "w_edit_A_hat_train = []\n",
    "w_gen_A_hat_train = []\n",
    "gen_A_raw_train = []\n",
    "gen_A_max_train = []\n",
    "gen_A_min_train = []\n",
    "masked_norm_A_hats = []\n",
    "\n",
    "for e in range(w_epochs):\n",
    "    for i in tqdm(range(len(batched_A_hat_discretized))):\n",
    "\n",
    "        fil = batched_gcn_filters_from_A_hat[i].float().to(device)\n",
    "        attr_hat = batched_Attr_hat[i].float().to(device)\n",
    "        # A_hat = batched_A_hat[i].to(device)\n",
    "        A_hat = batched_A_hat_discretized[i].to(device)\n",
    "        A = A_train[i]\n",
    "        z = batched_z[i].to(device)\n",
    "\n",
    "        ## discretize\n",
    "        # A = A_train[i].cpu().numpy().squeeze(-1)\n",
    "        # A_hat = A_hat.cpu().numpy().squeeze(-1)\n",
    "        # discretizer = Discretizer(A, A_hat)\n",
    "        # A_hat = discretizer.discretize('hard_threshold')\n",
    "        # A = torch.unsqueeze(torch.from_numpy(A), -1)\n",
    "        # A_hat = torch.unsqueeze(torch.from_numpy(A_hat), -1)\n",
    "\n",
    "#         _, alpha_edit = transform.get_train_alpha(A_hat)\n",
    "        _, alpha_edit = 0, 0.35\n",
    "        # alpha_gen = a_w2 * F.relu(a_w1 * alpha_edit + a_b1) + a_b2\n",
    "        sign = -1 if alpha_edit < 0 else 1\n",
    "        alpha_gen = sign * torch.log(torch.abs(torch.tensor(alpha_edit)))\n",
    "\n",
    "        ## first get edit and D(edit(G(z)))\n",
    "        \n",
    "        \n",
    "        edit_attr = attr_hat\n",
    "        edit_A = transform.get_target_graph(alpha_edit, A_hat, list(Param_train[i][:,-1].type(torch.LongTensor)))  # replace this with the edit(G(z)) attr & filter! Expect do all graphs in batch in one step!!\n",
    "        # print(alpha_edit, alpha_gen)\n",
    "\n",
    "#         temp = edit_A.detach().cpu()\n",
    "#         edit_fil = preprocess_adj_tensor_with_identity(torch.squeeze(temp, -1), symmetric = False).to(device)\n",
    "#         feature_edit, _ = discriminator(edit_attr.float(), edit_fil.float())\n",
    "\n",
    "\n",
    "        # Then get G(z + aw) and D(G(z + aw))\n",
    "    \n",
    "        gen_A, gen_attr, gen_A_raw, gen_A_max, gen_A_min = generator(z + alpha_gen * w)\n",
    "#         temp = gen_A.detach().cpu()\n",
    "#         gen_fil = preprocess_adj_tensor_with_identity(torch.squeeze(temp, -1), symmetric = False).to(device)\n",
    "#         feature_gen, preds = discriminator(gen_attr.float(), gen_fil.float())\n",
    "#         labels = torch.ones(edit_attr.shape[0]).to(device)\n",
    "\n",
    "\n",
    "\n",
    "        if e + 1 == w_epochs:\n",
    "            w_A_train.append(A)\n",
    "            w_A_hat_train.append(A_hat)\n",
    "            w_edit_A_hat_train.append(edit_A)\n",
    "            w_gen_A_hat_train.append(gen_A.detach())\n",
    "            gen_A_raw_train.append(gen_A_raw.detach())\n",
    "            masked_norm_A = masked_normalization(gen_A_raw.detach(), Param_train[i])\n",
    "            masked_norm_A_hats.append(masked_norm_A)\n",
    "            gen_A_max_train.append(gen_A_max.detach())\n",
    "            gen_A_min_train.append(gen_A_min.detach())\n",
    "\n",
    "print(\"====================== G(z + aw) v.s. edit(G(z))  results =============================\")\n",
    "gen_A_discretized = debugDiscretizer(w_A_hat_train, w_edit_A_hat_train, gen_A_raw_train, gen_A_max_train, gen_A_min_train, w_gen_A_hat_train, masked_norm_A_hats, discretize_method=\"hard_threshold\", printMatrix=False, abortPickle=True)\n",
    "\n",
    "\n",
    "#debugDecoder(w_edit_A_hat_train, [], w_gen_A_hat_train, [], discretize_method=\"hard_threshold\", printMatrix=True)\n",
    "# drawGraph(w_A_train, w_A_hat_train, w_edit_A_hat_train, w_gen_A_hat_train)\n",
    "# drawGraphSaveFigure(w_A_train, w_A_hat_train, w_edit_A_hat_train, w_gen_A_hat_train, clearImage=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Original topology (averaged) ---\n",
      " density: 0.23133233082706686 \n",
      " diameter: -1.0 \n",
      " clustering coefficient: 0.40446849661076467 \n",
      " edges: 43.95314285714286 \n",
      " avgerage degree 0.23133233082706683\n",
      "\n",
      "--- Edit topology (averaged) ---\n",
      " density: 0.3704706766917278 \n",
      " diameter: -0.9988571428571429 \n",
      " clustering coefficient: 0.5145780875193894 \n",
      " edges: 70.38942857142857 \n",
      " avgerage degree 0.3704706766917278\n",
      "\n",
      "--- Reconstructed topology (averaged) ---\n",
      " density: 0.2622511278195479 \n",
      " diameter: -1.0 \n",
      " clustering coefficient: 0.3949161427834552 \n",
      " edges: 49.827714285714286 \n",
      " avgerage degree 0.2622511278195479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "density_ori, diameter_ori, cluster_coef_ori, edges_ori, avg_degree_ori = topological_measure(batched_A_hat_discretized) # G(z)\n",
    "density_ed, diameter_ed, cluster_coef_ed, edges_ed, avg_degree_ed = topological_measure(w_edit_A_hat_train) # edit(G(z), edit_alpha)\n",
    "density_hat, diameter_hat, cluster_coef_hat, edges_hat, avg_degree_hat = topological_measure([torch.from_numpy(gen_A_discretized)]) # G(z + alpaha * w)\n",
    "print(f\"--- Original topology (averaged) ---\\n density: {density_ori} \\n diameter: {diameter_ori} \"\n",
    "      f\"\\n clustering coefficient: {cluster_coef_ori} \\n edges: {edges_ori} \\n avgerage degree {avg_degree_ori}\\n\")\n",
    "\n",
    "print(f\"--- Edit topology (averaged) ---\\n density: {density_ed} \\n diameter: {diameter_ed} \"\n",
    "      f\"\\n clustering coefficient: {cluster_coef_ed} \\n edges: {edges_ed} \\n avgerage degree {avg_degree_ed}\\n\")\n",
    "\n",
    "print(f\"--- Reconstructed topology (averaged) ---\\n density: {density_hat} \\n diameter: {diameter_hat} \"\n",
    "      f\"\\n clustering coefficient: {cluster_coef_hat} \\n edges: {edges_hat} \\n avgerage degree {avg_degree_hat}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!!!==========================edit_alpha = 0.01 ===================================!!!!!!!!!!!!!!\n",
      "Current discretize threshold: 0.45\n",
      "tensor(-4.6052)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|████████████                                                                        | 1/7 [00:00<00:02,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.4975826144218445\n",
      "Avaerge probability variance: 0.04734661057591438\n",
      "tensor(-4.6052)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|████████████████████████                                                            | 2/7 [00:01<00:02,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.4784378409385681\n",
      "Avaerge probability variance: 0.050205618143081665\n",
      "tensor(-4.6052)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████████████████████████████████████                                                | 3/7 [00:01<00:02,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.495572030544281\n",
      "Avaerge probability variance: 0.04906446859240532\n",
      "tensor(-4.6052)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|████████████████████████████████████████████████                                    | 4/7 [00:02<00:01,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.49648499488830566\n",
      "Avaerge probability variance: 0.04644932597875595\n",
      "tensor(-4.6052)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|████████████████████████████████████████████████████████████                        | 5/7 [00:02<00:01,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.49852973222732544\n",
      "Avaerge probability variance: 0.04667777568101883\n",
      "tensor(-4.6052)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████████████████████████████████████████████████████████████████████            | 6/7 [00:03<00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.49431169033050537\n",
      "Avaerge probability variance: 0.04676392674446106\n",
      "tensor(-4.6052)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:03<00:00,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.47086384892463684\n",
      "Avaerge probability variance: 0.04608330875635147\n",
      "====================== G(z + aw) v.s. edit(G(z))  results =============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9232457142857112\n",
      "precision: 0.7695010430667657\n",
      "recall: 0.6917284843771777\n",
      "f1 score: 0.7285450783058461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Original topology (averaged) ---\n",
      " density: 0.23133233082706686 \n",
      " diameter: -1.0 \n",
      " clustering coefficient: 0.40446849661076467 \n",
      " edges: 43.95314285714286 \n",
      " avgerage degree 0.23133233082706683\n",
      "\n",
      "--- Edit topology (averaged) ---\n",
      " density: 0.2960180451127809 \n",
      " diameter: -1.0 \n",
      " clustering coefficient: 0.41670935805324505 \n",
      " edges: 56.24342857142857 \n",
      " avgerage degree 0.2960180451127809\n",
      "\n",
      "--- Reconstructed topology (averaged) ---\n",
      " density: 0.2554436090225555 \n",
      " diameter: -1.0 \n",
      " clustering coefficient: 0.3920112143435472 \n",
      " edges: 48.534285714285716 \n",
      " avgerage degree 0.2554436090225555\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "!!!!!!!!!!!!!!==========================edit_alpha = 0.1 ===================================!!!!!!!!!!!!!!\n",
      "Current discretize threshold: 0.4\n",
      "tensor(-2.3026)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|████████████                                                                        | 1/7 [00:00<00:03,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.527472972869873\n",
      "Avaerge probability variance: 0.055917419493198395\n",
      "tensor(-2.3026)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|████████████████████████                                                            | 2/7 [00:01<00:02,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.5064720511436462\n",
      "Avaerge probability variance: 0.05733386054635048\n",
      "tensor(-2.3026)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████████████████████████████████████                                                | 3/7 [00:01<00:02,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.5231239199638367\n",
      "Avaerge probability variance: 0.056157976388931274\n",
      "tensor(-2.3026)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|████████████████████████████████████████████████                                    | 4/7 [00:02<00:01,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.521909773349762\n",
      "Avaerge probability variance: 0.053582701832056046\n",
      "tensor(-2.3026)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|████████████████████████████████████████████████████████████                        | 5/7 [00:02<00:01,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.5276616811752319\n",
      "Avaerge probability variance: 0.05280089005827904\n",
      "tensor(-2.3026)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████████████████████████████████████████████████████████████████████            | 6/7 [00:03<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.5257387757301331\n",
      "Avaerge probability variance: 0.052873238921165466\n",
      "tensor(-2.3026)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:03<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.4989129602909088\n",
      "Avaerge probability variance: 0.05146193131804466\n",
      "====================== G(z + aw) v.s. edit(G(z))  results =============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9338421428571401\n",
      "precision: 0.7938518261241804\n",
      "recall: 0.7528171346063793\n",
      "f1 score: 0.7727901344351832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Original topology (averaged) ---\n",
      " density: 0.23133233082706686 \n",
      " diameter: -1.0 \n",
      " clustering coefficient: 0.40446849661076467 \n",
      " edges: 43.95314285714286 \n",
      " avgerage degree 0.23133233082706683\n",
      "\n",
      "--- Edit topology (averaged) ---\n",
      " density: 0.3181909774436075 \n",
      " diameter: -1.0 \n",
      " clustering coefficient: 0.45291405300115345 \n",
      " edges: 60.45628571428571 \n",
      " avgerage degree 0.3181909774436075\n",
      "\n",
      "--- Reconstructed topology (averaged) ---\n",
      " density: 0.29598646616541224 \n",
      " diameter: -1.0 \n",
      " clustering coefficient: 0.4378644690113816 \n",
      " edges: 56.23742857142857 \n",
      " avgerage degree 0.29598646616541224\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "!!!!!!!!!!!!!!==========================edit_alpha = 0.2 ===================================!!!!!!!!!!!!!!\n",
      "Current discretize threshold: 0.35\n",
      "tensor(-1.6094)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|████████████                                                                        | 1/7 [00:00<00:03,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.5263524055480957\n",
      "Avaerge probability variance: 0.05775034427642822\n",
      "tensor(-1.6094)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|████████████████████████                                                            | 2/7 [00:01<00:02,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.5062267184257507\n",
      "Avaerge probability variance: 0.058510199189186096\n",
      "tensor(-1.6094)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████████████████████████████████████                                                | 3/7 [00:01<00:02,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.5220262408256531\n",
      "Avaerge probability variance: 0.05744999647140503\n",
      "tensor(-1.6094)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|████████████████████████████████████████████████                                    | 4/7 [00:02<00:01,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.520345151424408\n",
      "Avaerge probability variance: 0.05503770336508751\n",
      "tensor(-1.6094)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|████████████████████████████████████████████████████████████                        | 5/7 [00:02<00:01,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.5272032022476196\n",
      "Avaerge probability variance: 0.05380004644393921\n",
      "tensor(-1.6094)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████████████████████████████████████████████████████████████████████            | 6/7 [00:03<00:00,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.5248556137084961\n",
      "Avaerge probability variance: 0.0536915548145771\n",
      "tensor(-1.6094)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:03<00:00,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.49672335386276245\n",
      "Avaerge probability variance: 0.05237511545419693\n",
      "====================== G(z + aw) v.s. edit(G(z))  results =============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9341157142857108\n",
      "precision: 0.8182078030709219\n",
      "recall: 0.7750764222074056\n",
      "f1 score: 0.7960583134695998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Original topology (averaged) ---\n",
      " density: 0.23133233082706686 \n",
      " diameter: -1.0 \n",
      " clustering coefficient: 0.40446849661076467 \n",
      " edges: 43.95314285714286 \n",
      " avgerage degree 0.23133233082706683\n",
      "\n",
      "--- Edit topology (averaged) ---\n",
      " density: 0.34092481203007363 \n",
      " diameter: -1.0 \n",
      " clustering coefficient: 0.47854473543030185 \n",
      " edges: 64.77571428571429 \n",
      " avgerage degree 0.34092481203007363\n",
      "\n",
      "--- Reconstructed topology (averaged) ---\n",
      " density: 0.31899097744360744 \n",
      " diameter: -1.0 \n",
      " clustering coefficient: 0.46219026225408527 \n",
      " edges: 60.608285714285714 \n",
      " avgerage degree 0.31899097744360744\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "!!!!!!!!!!!!!!==========================edit_alpha = 0.3 ===================================!!!!!!!!!!!!!!\n",
      "Current discretize threshold: 0.3\n",
      "tensor(-1.2040)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|████████████                                                                        | 1/7 [00:00<00:03,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.5235359072685242\n",
      "Avaerge probability variance: 0.05848703905940056\n",
      "tensor(-1.2040)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|████████████████████████                                                            | 2/7 [00:01<00:02,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.5041661858558655\n",
      "Avaerge probability variance: 0.05888311192393303\n",
      "tensor(-1.2040)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████████████████████████████████████                                                | 3/7 [00:01<00:02,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.5195767283439636\n",
      "Avaerge probability variance: 0.057965077459812164\n",
      "tensor(-1.2040)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|████████████████████████████████████████████████                                    | 4/7 [00:02<00:01,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.5172445774078369\n",
      "Avaerge probability variance: 0.05547791346907616\n",
      "tensor(-1.2040)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|████████████████████████████████████████████████████████████                        | 5/7 [00:03<00:01,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.5247770547866821\n",
      "Avaerge probability variance: 0.054093506187200546\n",
      "tensor(-1.2040)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████████████████████████████████████████████████████████████████████            | 6/7 [00:03<00:00,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.5221695899963379\n",
      "Avaerge probability variance: 0.05385541543364525\n",
      "tensor(-1.2040)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:04<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.4929395020008087\n",
      "Avaerge probability variance: 0.052571333944797516\n",
      "====================== G(z + aw) v.s. edit(G(z))  results =============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9366607142857118\n",
      "precision: 0.8667024738820258\n",
      "recall: 0.811079724673951\n",
      "f1 score: 0.8379690814403552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Original topology (averaged) ---\n",
      " density: 0.23133233082706686 \n",
      " diameter: -1.0 \n",
      " clustering coefficient: 0.40446849661076467 \n",
      " edges: 43.95314285714286 \n",
      " avgerage degree 0.23133233082706683\n",
      "\n",
      "--- Edit topology (averaged) ---\n",
      " density: 0.3608270676691709 \n",
      " diameter: -1.0 \n",
      " clustering coefficient: 0.5002110915694945 \n",
      " edges: 68.55714285714286 \n",
      " avgerage degree 0.3608270676691709\n",
      "\n",
      "--- Reconstructed topology (averaged) ---\n",
      " density: 0.34293383458646465 \n",
      " diameter: -1.0 \n",
      " clustering coefficient: 0.48579418445233646 \n",
      " edges: 65.15742857142857 \n",
      " avgerage degree 0.3429338345864646\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "!!!!!!!!!!!!!!==========================edit_alpha = 0.4 ===================================!!!!!!!!!!!!!!\n",
      "Current discretize threshold: 0.25\n",
      "tensor(-0.9163)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|████████████                                                                        | 1/7 [00:00<00:04,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.5204788446426392\n",
      "Avaerge probability variance: 0.05886845663189888\n",
      "tensor(-0.9163)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|████████████████████████                                                            | 2/7 [00:01<00:03,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.5015603303909302\n",
      "Avaerge probability variance: 0.059074580669403076\n",
      "tensor(-0.9163)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████████████████████████████████████                                                | 3/7 [00:02<00:02,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.5168994069099426\n",
      "Avaerge probability variance: 0.05819296836853027\n",
      "tensor(-0.9163)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|████████████████████████████████████████████████                                    | 4/7 [00:02<00:02,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.5138733983039856\n",
      "Avaerge probability variance: 0.05572256073355675\n",
      "tensor(-0.9163)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|████████████████████████████████████████████████████████████                        | 5/7 [00:03<00:01,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.521710991859436\n",
      "Avaerge probability variance: 0.054216280579566956\n",
      "tensor(-0.9163)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████████████████████████████████████████████████████████████████████            | 6/7 [00:04<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.5190054774284363\n",
      "Avaerge probability variance: 0.05389263480901718\n",
      "tensor(-0.9163)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:04<00:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.48921310901641846\n",
      "Avaerge probability variance: 0.052588220685720444\n",
      "====================== G(z + aw) v.s. edit(G(z))  results =============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9434042857142826\n",
      "precision: 0.9058785391480134\n",
      "recall: 0.8502022248999739\n",
      "f1 score: 0.8771577768409892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Original topology (averaged) ---\n",
      " density: 0.23133233082706686 \n",
      " diameter: -1.0 \n",
      " clustering coefficient: 0.40446849661076467 \n",
      " edges: 43.95314285714286 \n",
      " avgerage degree 0.23133233082706683\n",
      "\n",
      "--- Edit topology (averaged) ---\n",
      " density: 0.37941804511278004 \n",
      " diameter: -0.9988571428571429 \n",
      " clustering coefficient: 0.5235466862689119 \n",
      " edges: 72.08942857142857 \n",
      " avgerage degree 0.37941804511278004\n",
      "\n",
      "--- Reconstructed topology (averaged) ---\n",
      " density: 0.3655593984962378 \n",
      " diameter: -1.0 \n",
      " clustering coefficient: 0.5081806931117453 \n",
      " edges: 69.45628571428571 \n",
      " avgerage degree 0.3655593984962378\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "!!!!!!!!!!!!!!==========================edit_alpha = 0.5 ===================================!!!!!!!!!!!!!!\n",
      "Current discretize threshold: 0.19999999999999996\n",
      "tensor(-0.6931)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|████████████                                                                        | 1/7 [00:00<00:03,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.5171306729316711\n",
      "Avaerge probability variance: 0.05928654596209526\n",
      "tensor(-0.6931)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|████████████████████████                                                            | 2/7 [00:01<00:03,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.49832552671432495\n",
      "Avaerge probability variance: 0.0593380369246006\n",
      "tensor(-0.6931)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████████████████████████████████████                                                | 3/7 [00:02<00:02,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.5137918591499329\n",
      "Avaerge probability variance: 0.05845586583018303\n",
      "tensor(-0.6931)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|████████████████████████████████████████████████                                    | 4/7 [00:02<00:01,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.5101432800292969\n",
      "Avaerge probability variance: 0.056083794683218\n",
      "tensor(-0.6931)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|████████████████████████████████████████████████████████████                        | 5/7 [00:03<00:01,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.5181131362915039\n",
      "Avaerge probability variance: 0.05454786494374275\n",
      "tensor(-0.6931)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████████████████████████████████████████████████████████████████████            | 6/7 [00:04<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.5156394839286804\n",
      "Avaerge probability variance: 0.05410677567124367\n",
      "tensor(-0.6931)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:04<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaerge probability: 0.48545169830322266\n",
      "Avaerge probability variance: 0.052768439054489136\n",
      "====================== G(z + aw) v.s. edit(G(z))  results =============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9525035714285667\n",
      "precision: 0.9201901681677563\n",
      "recall: 0.8987107225817093\n",
      "f1 score: 0.9093236197227607\n",
      "--- Original topology (averaged) ---\n",
      " density: 0.23133233082706686 \n",
      " diameter: -1.0 \n",
      " clustering coefficient: 0.40446849661076467 \n",
      " edges: 43.95314285714286 \n",
      " avgerage degree 0.23133233082706683\n",
      "\n",
      "--- Edit topology (averaged) ---\n",
      " density: 0.3948075187969905 \n",
      " diameter: -0.9991428571428571 \n",
      " clustering coefficient: 0.5399600073358988 \n",
      " edges: 75.01342857142858 \n",
      " avgerage degree 0.3948075187969905\n",
      "\n",
      "--- Reconstructed topology (averaged) ---\n",
      " density: 0.3837699248120277 \n",
      " diameter: -1.0 \n",
      " clustering coefficient: 0.5285956397360818 \n",
      " edges: 72.91628571428572 \n",
      " avgerage degree 0.3837699248120277\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "global_norm_A = []\n",
    "global_edit_A = []\n",
    "for idx,alpha in enumerate([0.01, 0.1, 0.2, 0.3, 0.4, 0.5]):\n",
    "    print(f\"!!!!!!!!!!!!!!==========================edit_alpha = {alpha} ===================================!!!!!!!!!!!!!!\")\n",
    "    loss_train = []\n",
    "    w_A_train = []\n",
    "    w_A_hat_train = []\n",
    "    w_edit_A_hat_train = []\n",
    "    w_gen_A_hat_train = []\n",
    "    gen_A_raw_train = []\n",
    "    gen_A_max_train = []\n",
    "    gen_A_min_train = []\n",
    "    masked_norm_A_hats = []\n",
    "    threshold = 0.5 - 0.05 * (idx + 1)\n",
    "    print(f\"Current discretize threshold: {threshold}\")\n",
    "    for e in range(w_epochs):\n",
    "        for i in tqdm(range(len(batched_A_hat_discretized))):\n",
    "\n",
    "            fil = batched_gcn_filters_from_A_hat[i].float().to(device)\n",
    "            attr_hat = batched_Attr_hat[i].float().to(device)\n",
    "            # A_hat = batched_A_hat[i].to(device)\n",
    "            A_hat = batched_A_hat_discretized[i].to(device)\n",
    "            A = A_train[i]\n",
    "            z = batched_z[i].to(device)\n",
    "\n",
    "            _, alpha_edit = 0, alpha\n",
    "        \n",
    "            sign = -1 if alpha_edit < 0 else 1\n",
    "            alpha_gen = sign * torch.log(torch.abs(torch.tensor(alpha_edit)))\n",
    "            print(alpha_gen)\n",
    "\n",
    "            edit_attr = attr_hat\n",
    "            edit_A = transform.get_target_graph(alpha_edit, A_hat, list(Param_train[i][:,-1].type(torch.LongTensor)))  # replace this with the edit(G(z)) attr & filter! Expect do all graphs in batch in one step!!\n",
    "\n",
    "            gen_A, gen_attr, gen_A_raw, gen_A_max, gen_A_min = generator(z + alpha_gen * w)\n",
    "\n",
    "            if e + 1 == w_epochs:\n",
    "                w_A_train.append(A)\n",
    "                w_A_hat_train.append(A_hat)\n",
    "                w_edit_A_hat_train.append(edit_A)\n",
    "                w_gen_A_hat_train.append(gen_A.detach())\n",
    "                gen_A_raw_train.append(gen_A_raw.detach())\n",
    "                masked_norm_A = masked_normalization(gen_A_raw.detach(), Param_train[i])\n",
    "                masked_norm_A_hats.append(masked_norm_A)\n",
    "                print(f\"Avaerge probability: {torch.mean(masked_norm_A[masked_norm_A > 1e-3])}\")\n",
    "                print(f\"Avaerge probability variance: {torch.var(masked_norm_A[masked_norm_A > 1e-3])}\")\n",
    "                gen_A_max_train.append(gen_A_max.detach())\n",
    "                gen_A_min_train.append(gen_A_min.detach())\n",
    "                \n",
    "        global_norm_A.append(masked_norm_A_hats)\n",
    "        global_edit_A.append(w_edit_A_hat_train)\n",
    "\n",
    "\n",
    "    print(\"====================== G(z + aw) v.s. edit(G(z))  results =============================\")\n",
    "    gen_A_discretized = debugDiscretizer(w_A_hat_train, w_edit_A_hat_train, gen_A_raw_train, gen_A_max_train, gen_A_min_train, w_gen_A_hat_train, masked_norm_A_hats, discretize_method=\"hard_threshold\", printMatrix=False, abortPickle=True, threshold=threshold)\n",
    "\n",
    "    density_ori, diameter_ori, cluster_coef_ori, edges_ori, avg_degree_ori = topological_measure(batched_A_hat_discretized) # G(z)\n",
    "    density_ed, diameter_ed, cluster_coef_ed, edges_ed, avg_degree_ed = topological_measure(w_edit_A_hat_train) # edit(G(z), edit_alpha)\n",
    "    density_hat, diameter_hat, cluster_coef_hat, edges_hat, avg_degree_hat = topological_measure([torch.from_numpy(gen_A_discretized)]) # G(z + alpaha * w)\n",
    "    print(f\"--- Original topology (averaged) ---\\n density: {density_ori} \\n diameter: {diameter_ori} \"\n",
    "          f\"\\n clustering coefficient: {cluster_coef_ori} \\n edges: {edges_ori} \\n avgerage degree {avg_degree_ori}\\n\")\n",
    "\n",
    "    print(f\"--- Edit topology (averaged) ---\\n density: {density_ed} \\n diameter: {diameter_ed} \"\n",
    "          f\"\\n clustering coefficient: {cluster_coef_ed} \\n edges: {edges_ed} \\n avgerage degree {avg_degree_ed}\\n\")\n",
    "\n",
    "    print(f\"--- Reconstructed topology (averaged) ---\\n density: {density_hat} \\n diameter: {diameter_hat} \"\n",
    "          f\"\\n clustering coefficient: {cluster_coef_hat} \\n edges: {edges_hat} \\n avgerage degree {avg_degree_hat}\\n\")\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\torch\\csrc\\utils\\python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0],\n",
       "        [  0,   0,   1],\n",
       "        [  0,   0,   2],\n",
       "        ...,\n",
       "        [427,   5,   3],\n",
       "        [427,   5,   4],\n",
       "        [427,   5,   5]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero(masked_norm_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4855)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(masked_norm_A[masked_norm_A > 1e-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compare(i1,i2,i3):\n",
    "    print(global_norm_A[i1][i2][i3].numpy())\n",
    "    print(global_edit_A[i1][i2][i3].squeeze(-1).numpy().astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.29 0.25 0.16 0.19 0.27 0.19 0.09 0.09 0.28 0.16 0.10 0.03 0.14 0.09 0.10 0.00 0.00 0.00 0.00 0.00]\n",
      " [0.25 0.35 0.14 0.20 0.18 0.09 0.03 0.07 0.37 0.09 0.06 0.00 0.19 0.17 0.13 0.00 0.00 0.00 0.00 0.00]\n",
      " [0.16 0.14 0.22 0.14 0.20 0.25 0.11 0.15 0.19 0.14 0.18 0.03 0.04 0.08 0.19 0.00 0.00 0.00 0.00 0.00]\n",
      " [0.19 0.20 0.14 0.22 0.17 0.14 0.05 0.16 0.27 0.10 0.12 0.02 0.16 0.16 0.21 0.00 0.00 0.00 0.00 0.00]\n",
      " [0.27 0.18 0.20 0.17 0.42 0.36 0.19 0.14 0.28 0.32 0.23 0.07 0.10 0.19 0.23 0.00 0.00 0.00 0.00 0.00]\n",
      " [0.19 0.09 0.25 0.14 0.36 0.45 0.23 0.22 0.19 0.30 0.32 0.09 0.02 0.15 0.29 0.00 0.00 0.00 0.00 0.00]\n",
      " [0.09 0.03 0.11 0.05 0.19 0.23 0.15 0.10 0.08 0.17 0.15 0.06 0.01 0.06 0.12 0.00 0.00 0.00 0.00 0.00]\n",
      " [0.09 0.07 0.15 0.16 0.14 0.22 0.10 0.25 0.14 0.12 0.25 0.05 0.08 0.17 0.34 0.00 0.00 0.00 0.00 0.00]\n",
      " [0.28 0.37 0.19 0.27 0.28 0.19 0.08 0.14 0.56 0.19 0.17 0.04 0.24 0.36 0.36 0.00 0.00 0.00 0.00 0.00]\n",
      " [0.16 0.09 0.14 0.10 0.32 0.30 0.17 0.12 0.19 0.30 0.22 0.07 0.04 0.18 0.25 0.00 0.00 0.00 0.00 0.00]\n",
      " [0.10 0.06 0.18 0.12 0.23 0.32 0.15 0.25 0.17 0.22 0.38 0.08 0.05 0.27 0.50 0.00 0.00 0.00 0.00 0.00]\n",
      " [0.03 0.00 0.03 0.02 0.07 0.09 0.06 0.05 0.04 0.07 0.08 0.04 0.02 0.06 0.09 0.00 0.00 0.00 0.00 0.00]\n",
      " [0.14 0.19 0.04 0.16 0.10 0.02 0.01 0.08 0.24 0.04 0.05 0.02 0.26 0.28 0.22 0.00 0.00 0.00 0.00 0.00]\n",
      " [0.09 0.17 0.08 0.16 0.19 0.15 0.06 0.17 0.36 0.18 0.27 0.06 0.28 0.76 0.69 0.00 0.00 0.00 0.00 0.00]\n",
      " [0.10 0.13 0.19 0.21 0.23 0.29 0.12 0.34 0.36 0.25 0.50 0.09 0.22 0.69 0.97 0.00 0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]]\n",
      "[[1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "compare(0,0,449)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.6931)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=150)\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
