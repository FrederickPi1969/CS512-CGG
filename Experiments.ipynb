{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                                                              | 27/5000 [00:00<00:19, 251.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cpu\n",
      "\n",
      "============= Generating Data ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:14<00:00, 340.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from model import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from utils_for_experiments import *\n",
    "# from transform_wrappers_multiprocessing import *\n",
    "from transform_wrappers import *\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from discretize import *\n",
    "from visualization import *\n",
    "\n",
    "\n",
    "np.set_printoptions(linewidth=np.inf)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device = \"cpu\"\n",
    "    print(f\"using device {device}\")\n",
    "\n",
    "    ####################     Generation parameters     #######################################################\n",
    "    dataArgs = dict()\n",
    "\n",
    "    maximum_number_of_nodes_n = \"20\" #@param [12, 24, 30, 48]\n",
    "    dataArgs[\"max_n_node\"] = int(maximum_number_of_nodes_n)\n",
    "\n",
    "    range_of_linkage_probability_p = \"0.0, 1.0\" #@param [[0.0,1.0], [0.2,0.8], [0.5,0.5]]\n",
    "    dataArgs[\"p_range\"] = [float(range_of_linkage_probability_p.split(\",\")[0]), float(range_of_linkage_probability_p.split(\",\")[1])]\n",
    "\n",
    "    node_attributes = \"degree\" #@param [\"uniform\", \"degree\", \"random\"]\n",
    "    dataArgs[\"node_attr\"] = node_attributes\n",
    "\n",
    "    number_of_graph_instances = \"5000\" #@param [1, 100, 1000, 10000, 25000, 50000, 100000, 200000, 500000, 1000000]\n",
    "    dataArgs[\"n_graph\"] = int(number_of_graph_instances)\n",
    "\n",
    "    dataArgs[\"upper_triangular\"] = False\n",
    "    A, Attr, Param, Topol = generate_data_v2(dataArgs)\n",
    "    # g, a, attr = unpad_data(A[0], Attr[0])\n",
    "\n",
    "    ####################     Model parameters     #######################################################\n",
    "    modelArgs = {\"gnn_filters\": 2, \"conv_filters\": 16, \"kernel_size\": 3}\n",
    "\n",
    "    number_of_latent_variables= \"20\" #@param [1, 2, 3, 4, 5]\n",
    "    modelArgs[\"latent_dim\"] = int(number_of_latent_variables)\n",
    "\n",
    "    trainArgs = dict()\n",
    "\n",
    "    weight_graph_reconstruction_loss = \"30\" #@param [0, 1, 2, 3, 5, 10, 20]\n",
    "    weight_attribute_reconstruction_loss = \"5\" #@param [0, 1, 2, 3, 5, 10, 20]\n",
    "    beta_value = \"20\" #@param [0, 1, 2, 3, 5, 10, 20]\n",
    "    trainArgs[\"loss_weights\"] = [int(weight_graph_reconstruction_loss), int(weight_attribute_reconstruction_loss), int(beta_value)]\n",
    "\n",
    "    epochs = \"35\" #@param [10, 20, 50]\n",
    "    trainArgs[\"epochs\"] = int(epochs)\n",
    "    batch_size = \"512\" #@param [2, 4, 8, 16, 32, 128, 512, 1024]\n",
    "    trainArgs[\"batch_size\"] = int(batch_size)\n",
    "    early_stop = \"2\" #@param [1, 2, 3, 4, 10]\n",
    "    trainArgs[\"early_stop\"] = int(early_stop)\n",
    "    train_test_split = \"0.2\" #@param [0.1, 0.2, 0.3, 0.5]\n",
    "    train_validation_split = \"0.1\" #@param [0.1, 0.2, 0.3, 0.5]\n",
    "    trainArgs[\"data_split\"] = float(train_test_split)\n",
    "    trainArgs[\"validation_split\"] = float(train_validation_split)\n",
    "    lr = \"0.001\"  #@param [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "    trainArgs[\"lr\"] = float(lr)\n",
    "\n",
    "\n",
    "\n",
    "    ## Train and Test Split _______________________________________________\n",
    "\n",
    "    A_train = torch.from_numpy(A[:int((1-trainArgs[\"data_split\"]-trainArgs[\"validation_split\"])*A.shape[0])])\n",
    "    Attr_train = generate_batch(torch.from_numpy(Attr[:int((1-trainArgs[\"data_split\"]-trainArgs[\"validation_split\"])*Attr.shape[0])]), trainArgs[\"batch_size\"])\n",
    "    Param_train = generate_batch(torch.from_numpy(Param[:int((1-trainArgs[\"data_split\"]-trainArgs[\"validation_split\"])*Param.shape[0])]), trainArgs[\"batch_size\"])\n",
    "    Topol_train = generate_batch(torch.from_numpy(Topol[:int((1-trainArgs[\"data_split\"]-trainArgs[\"validation_split\"])*Topol.shape[0])]), trainArgs[\"batch_size\"])\n",
    "\n",
    "    A_validate = torch.from_numpy(A[int((1-trainArgs[\"data_split\"]-trainArgs[\"validation_split\"])*A.shape[0]):int((1-trainArgs[\"data_split\"])*Attr.shape[0])])\n",
    "    Attr_validate = generate_batch(torch.from_numpy(Attr[int((1-trainArgs[\"data_split\"]-trainArgs[\"validation_split\"])*Attr.shape[0]):int((1-trainArgs[\"data_split\"])*Attr.shape[0])]), trainArgs[\"batch_size\"])\n",
    "    Param_validate = generate_batch(torch.from_numpy(Param[int((1-trainArgs[\"data_split\"]-trainArgs[\"validation_split\"])*Param.shape[0]):int((1-trainArgs[\"data_split\"])*Attr.shape[0])]), trainArgs[\"batch_size\"])\n",
    "    Topol_validate = generate_batch(torch.from_numpy(Topol[int((1-trainArgs[\"data_split\"]-trainArgs[\"validation_split\"])*Topol.shape[0]):int((1-trainArgs[\"data_split\"])*Attr.shape[0])]), trainArgs[\"batch_size\"])\n",
    "\n",
    "    A_test = torch.from_numpy(A[int((1-trainArgs[\"data_split\"])*A.shape[0]):])\n",
    "    Attr_test = generate_batch(torch.from_numpy(Attr[int((1-trainArgs[\"data_split\"])*Attr.shape[0]):]), trainArgs[\"batch_size\"])\n",
    "    Param_test = generate_batch(torch.from_numpy(Param[int((1-trainArgs[\"data_split\"])*Param.shape[0]):]), trainArgs[\"batch_size\"])\n",
    "    Topol_test = generate_batch(torch.from_numpy(Topol[int((1-trainArgs[\"data_split\"])*Topol.shape[0]):]), trainArgs[\"batch_size\"])\n",
    "\n",
    "\n",
    "    # print(A_train.shape)\n",
    "    # print(len(Attr_train), Attr_train[0].shape)\n",
    "\n",
    "    ## build graph_conv_filters\n",
    "    SYM_NORM = True\n",
    "    A_train_mod = generate_batch(preprocess_adj_tensor_with_identity(torch.squeeze(A_train, -1), SYM_NORM), trainArgs[\"batch_size\"])\n",
    "    A_validate_mod = generate_batch(preprocess_adj_tensor_with_identity(torch.squeeze(A_validate, -1), SYM_NORM), trainArgs[\"batch_size\"])\n",
    "    A_test_mod = generate_batch(preprocess_adj_tensor_with_identity(torch.squeeze(A_test, -1), SYM_NORM), trainArgs[\"batch_size\"])\n",
    "\n",
    "    A_train = generate_batch(A_train, trainArgs[\"batch_size\"])\n",
    "    A_validate = generate_batch(A_validate, trainArgs[\"batch_size\"])\n",
    "    A_test = generate_batch(A_test, trainArgs[\"batch_size\"])\n",
    "\n",
    "    train_data = (Attr_train, A_train_mod, Param_train, Topol_train)\n",
    "    validate_data = (Attr_validate, A_validate_mod, Param_validate, Topol_validate)\n",
    "    test_data = (Attr_test, A_test_mod, Param_test, Topol_test)\n",
    "\n",
    "    # attribute first -> (n, n), adjacency second -> (n, n, 1)\n",
    "    modelArgs[\"input_shape\"], modelArgs[\"output_shape\"] = ((Attr_train[0].shape[1], Attr_train[0].shape[2]), (int(A_train_mod[0].shape[1] / modelArgs[\"gnn_filters\"]), A_train_mod[0].shape[2], 1)),\\\n",
    "                                                          ((Attr_test[0].shape[1], Attr_test[0].shape[1]), (int(A_test_mod[0].shape[1] / modelArgs[\"gnn_filters\"]), A_test_mod[0].shape[2], 1))\n",
    "    # print(modelArgs[\"input_shape\"], modelArgs[\"output_shape\"])\n",
    "    # print(A_train[0].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " =================Extracting useful information=====================\n",
      "Model loss 2397.4097726004466 \n",
      "VAE performance:  \n",
      " Accuracy: 0.913662857142848,  \n",
      " Precision: 0.7448206334619567,  \n",
      " Recall: 0.8284053918177283,  \n",
      " F1 Score: 0.7843926032018056\n",
      "\n",
      "--- Truth topology (averaged) ---\n",
      " density: 0.2522917293233071 \n",
      " diameter: -1.0 \n",
      " clustering coefficient: 0.27912696762154776 \n",
      " edges: 47.935428571428574 \n",
      " avgerage degree 0.2522917293233071\n",
      "\n",
      "--- Reconstructed topology (averaged) ---\n",
      " density: 0.29779849624059956 \n",
      " diameter: -1.0 \n",
      " clustering coefficient: 0.4116737698692536 \n",
      " edges: 56.581714285714284 \n",
      " avgerage degree 0.29779849624059956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "######################      VAE        ##########################################\n",
    "operation_name = \"density\"  ## [\"transitivity\", \"density\", \"forest fire ...\"]\n",
    "# param_path = operation_name + \"_pretrained\" + \"_\" + maximum_number_of_nodes_n\n",
    "param_path = \".\"\n",
    "vae = torch.load(param_path + \"/vae.model\")\n",
    "\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "batched_z = []\n",
    "batched_A_hat = []\n",
    "batched_Attr_hat = []\n",
    "batched_A_hat_discretized = []\n",
    "batched_A_hat_discretized_test = []\n",
    "batched_gcn_filters_from_A_hat = []\n",
    "batched_z_test = []\n",
    "batched_A_hat_test = []\n",
    "batched_Attr_hat_test = []\n",
    "batched_gcn_filters_from_A_hat_test = []\n",
    "batched_A_hat_raw_train = []\n",
    "batched_A_hat_raw_test = []\n",
    "batched_A_hat_max_train = []\n",
    "batched_A_hat_max_test = []\n",
    "batched_A_hat_min_train = []\n",
    "batched_A_hat_min_test = []\n",
    "print(\"\\n\\n =================Extracting useful information=====================\")\n",
    "vae.eval()\n",
    "\n",
    "def index_of(my_list, target):\n",
    "    try: return my_list.index(target)\n",
    "    except: return dataArgs[\"max_n_node\"]\n",
    "\n",
    "for e in range(1):\n",
    "    loss_cum = 0\n",
    "    for i in range(len(Attr_train)):\n",
    "        attr = Attr_train[i].float().to(device)\n",
    "        A = A_train[i].float().to(device)\n",
    "        graph_conv_filters = A_train_mod[i].float().to(device)\n",
    "\n",
    "        z, z_mean, z_log_var, A_hat, attr_hat, A_hat_raw, max_score_per_node, min_score_per_node = vae(attr, graph_conv_filters)\n",
    "\n",
    "        if e + 1 == 1:\n",
    "            batched_z.append(z.detach())\n",
    "            batched_Attr_hat.append(attr_hat.detach())\n",
    "            batched_A_hat.append(A_hat.detach())\n",
    "            temp = A_hat.detach().cpu()\n",
    "            batched_gcn_filters_from_A_hat.append(preprocess_adj_tensor_with_identity(torch.squeeze(temp, -1), symmetric = False))\n",
    "\n",
    "            A_discretize = A.cpu().squeeze().numpy()\n",
    "            A_hat_discretize = A_hat.detach().cpu().squeeze().numpy()\n",
    "            discretizer = Discretizer(A_discretize, A_hat_discretize)\n",
    "            A_hat_discretize = discretizer.discretize('hard_threshold')\n",
    "            A_hat_discretize = torch.unsqueeze(torch.from_numpy(A_hat_discretize), -1)\n",
    "\n",
    "            batched_A_hat_discretized.append(A_hat_discretize)\n",
    "            batched_A_hat_raw_train.append(A_hat_raw.detach())\n",
    "            batched_A_hat_max_train.append(max_score_per_node.detach())\n",
    "            batched_A_hat_min_train.append(min_score_per_node.detach())\n",
    "\n",
    "            # count = 0\n",
    "            for j in range(len(batched_A_hat_discretized[i])):\n",
    "                temp = list(torch.diag(batched_A_hat_discretized[i][j].detach().reshape(dataArgs[\"max_n_node\"], -1)))[::-1]\n",
    "                pred_node_num = dataArgs[\"max_n_node\"] - index_of(list(temp), 1)\n",
    "                Param_train[i][j][-1] = pred_node_num  # predicted node num have ~96% acc\n",
    "                true_node_num = int(Param_train[i][j][0])\n",
    "                # print(pred_node_num)\n",
    "                # print(true_node_num)\n",
    "\n",
    "                # count += pred_node_num == true_node_num\n",
    "            # print(f\"node prediction accuracy : {count / len(batched_A_hat_discretized[i])}\")\n",
    "\n",
    "        loss = loss_func((A, attr), (A_hat, attr_hat), z_mean, z_log_var, trainArgs, modelArgs)\n",
    "        loss_cum += loss.item()\n",
    "\n",
    "    print(\"Model loss {} \".format(loss_cum / len(Attr_train)))\n",
    "\n",
    "\n",
    "    loss_cum = 0\n",
    "    for i in range(len(Attr_validate)):\n",
    "        attr = Attr_validate[i].float().to(device)\n",
    "        A = A_validate[i].float().to(device)\n",
    "        graph_conv_filters = A_validate_mod[i].float().to(device)\n",
    "\n",
    "        z, z_mean, z_log_var, A_hat, attr_hat, A_hat_raw, max_score_per_node, min_score_per_node = vae(attr, graph_conv_filters)\n",
    "        loss = loss_func((A, attr), (A_hat, attr_hat), z_mean, z_log_var, trainArgs, modelArgs)\n",
    "        loss_cum += loss.item()\n",
    "\n",
    "        if e + 1 == 1:\n",
    "            batched_z_test.append(z.detach())\n",
    "            batched_Attr_hat_test.append(attr_hat.detach())\n",
    "            batched_A_hat_test.append(A_hat.detach())\n",
    "            temp = A_hat.detach().cpu()\n",
    "            batched_gcn_filters_from_A_hat_test.append(preprocess_adj_tensor_with_identity(torch.squeeze(temp, -1), symmetric = False))\n",
    "\n",
    "            A_discretize = A.cpu().squeeze().numpy()\n",
    "            A_hat_discretize = A_hat.detach().cpu().squeeze().numpy()\n",
    "            discretizer = Discretizer(A_discretize, A_hat_discretize)\n",
    "            A_hat_discretize = discretizer.discretize('hard_threshold', threshold=0.7)\n",
    "            A_hat_discretize = torch.unsqueeze(torch.from_numpy(A_hat_discretize), -1)\n",
    "\n",
    "            batched_A_hat_discretized_test.append(A_hat_discretize)\n",
    "            batched_A_hat_raw_test.append(A_hat_raw.detach())\n",
    "            batched_A_hat_max_test.append(max_score_per_node.detach())\n",
    "            batched_A_hat_min_test.append(min_score_per_node.detach())\n",
    "\n",
    "    # print(\"At Epoch {}, validation loss {} \".format(e + 1, loss_cum / len(Attr_validate)))\n",
    "\n",
    "a,p,r,f = compute_score_batched(A_train, batched_A_hat_discretized)\n",
    "print(f\"VAE performance:  \\n Accuracy: {a},  \\n Precision: {p},  \\n Recall: {r},  \\n F1 Score: {f}\\n\")\n",
    "\n",
    "density_ori, diameter_ori, cluster_coef_ori, edges_ori, avg_degree_ori = topological_measure(A_train)\n",
    "density_hat, diameter_hat, cluster_coef_hat, edges_hat, avg_degree_hat = topological_measure(batched_A_hat_discretized)\n",
    "print(f\"--- Truth topology (averaged) ---\\n density: {density_ori} \\n diameter: {diameter_ori} \"\n",
    "      f\"\\n clustering coefficient: {cluster_coef_ori} \\n edges: {edges_ori} \\n avgerage degree {avg_degree_ori}\\n\")\n",
    "print(f\"--- Reconstructed topology (averaged) ---\\n density: {density_hat} \\n diameter: {diameter_hat} \"\n",
    "      f\"\\n clustering coefficient: {cluster_coef_hat} \\n edges: {edges_hat} \\n avgerage degree {avg_degree_hat}\\n\")\n",
    "\n",
    "\n",
    "################ Load Discriminator #####################################\n",
    "discriminator = torch.load(param_path + \"/discriminator.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################# Steering GAN   ####################################\n",
    "\n",
    "## training tip: same batch, same alpha!\n",
    "\n",
    "# w = torch.randn_like(batched_z[0][0], requires_grad=True).unsqueeze(0).to(device)\n",
    "w = torch.load(param_path + \"/w_density.pt\")\n",
    "a_w1 = torch.load(param_path + \"/a_w1_density.pt\")\n",
    "a_w2 = torch.load(param_path + \"/a_w2_density.pt\")\n",
    "a_b1 = torch.load(param_path + \"/a_b1_density.pt\")\n",
    "a_b2 = torch.load(param_path + \"/a_b2_density.pt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Initialize generator\n",
    "generator = Decoder_v2(modelArgs, trainArgs, device).to(device)\n",
    "\n",
    "decoder_weight = dict(vae.decoder.named_parameters())\n",
    "generator_weight = dict(generator.named_parameters())\n",
    "for k in generator_weight.keys():\n",
    "    assert k in decoder_weight\n",
    "    generator_weight[k] = decoder_weight[k]\n",
    "generator.eval()\n",
    "\n",
    "\n",
    "discriminator.eval()\n",
    "## operation = \"transitivity\", \"density\", \"node_count\"\n",
    "\n",
    "# transform = GraphTransform(dataArgs[\"max_n_node\"], operation = operation_name, sigmoid = False)\n",
    "transform = GraphTransform(dataArgs[\"max_n_node\"], operation = operation_name, sigmoid = False)\n",
    "w_epochs = 1  ### adjust epoch here!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:03<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================== G(z + aw) v.s. edit(G(z))  results =============================\n",
      "accuracy: 0.9374064285714244\n",
      "precision: 0.8457021869114075\n",
      "recall: 0.867570457009606\n",
      "f1 score: 0.8564967582901287\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_train = []\n",
    "w_A_train = []\n",
    "w_A_hat_train = []\n",
    "w_edit_A_hat_train = []\n",
    "w_gen_A_hat_train = []\n",
    "gen_A_raw_train = []\n",
    "gen_A_max_train = []\n",
    "gen_A_min_train = []\n",
    "masked_norm_A_hats = []\n",
    "\n",
    "for e in range(w_epochs):\n",
    "    for i in tqdm(range(len(batched_A_hat_discretized))):\n",
    "\n",
    "        fil = batched_gcn_filters_from_A_hat[i].float().to(device)\n",
    "        attr_hat = batched_Attr_hat[i].float().to(device)\n",
    "        # A_hat = batched_A_hat[i].to(device)\n",
    "        A_hat = batched_A_hat_discretized[i].to(device)\n",
    "        A = A_train[i]\n",
    "        z = batched_z[i].to(device)\n",
    "\n",
    "        ## discretize\n",
    "        # A = A_train[i].cpu().numpy().squeeze(-1)\n",
    "        # A_hat = A_hat.cpu().numpy().squeeze(-1)\n",
    "        # discretizer = Discretizer(A, A_hat)\n",
    "        # A_hat = discretizer.discretize('hard_threshold')\n",
    "        # A = torch.unsqueeze(torch.from_numpy(A), -1)\n",
    "        # A_hat = torch.unsqueeze(torch.from_numpy(A_hat), -1)\n",
    "\n",
    "#         _, alpha_edit = transform.get_train_alpha(A_hat)\n",
    "        _, alpha_edit = 0, 0.2\n",
    "        # alpha_gen = a_w2 * F.relu(a_w1 * alpha_edit + a_b1) + a_b2\n",
    "        sign = -1 if alpha_edit < 0 else 1\n",
    "        alpha_gen = sign * torch.log(torch.abs(torch.tensor(alpha_edit)))\n",
    "\n",
    "        ## first get edit and D(edit(G(z)))\n",
    "        \n",
    "        \n",
    "        edit_attr = attr_hat\n",
    "        edit_A = transform.get_target_graph(alpha_edit, A_hat, list(Param_train[i][:,-1].type(torch.LongTensor)))  # replace this with the edit(G(z)) attr & filter! Expect do all graphs in batch in one step!!\n",
    "        # print(alpha_edit, alpha_gen)\n",
    "\n",
    "#         temp = edit_A.detach().cpu()\n",
    "#         edit_fil = preprocess_adj_tensor_with_identity(torch.squeeze(temp, -1), symmetric = False).to(device)\n",
    "#         feature_edit, _ = discriminator(edit_attr.float(), edit_fil.float())\n",
    "\n",
    "\n",
    "        # Then get G(z + aw) and D(G(z + aw))\n",
    "    \n",
    "        gen_A, gen_attr, gen_A_raw, gen_A_max, gen_A_min = generator(z + alpha_gen * w)\n",
    "#         temp = gen_A.detach().cpu()\n",
    "#         gen_fil = preprocess_adj_tensor_with_identity(torch.squeeze(temp, -1), symmetric = False).to(device)\n",
    "#         feature_gen, preds = discriminator(gen_attr.float(), gen_fil.float())\n",
    "#         labels = torch.ones(edit_attr.shape[0]).to(device)\n",
    "\n",
    "\n",
    "\n",
    "        if e + 1 == w_epochs:\n",
    "            w_A_train.append(A)\n",
    "            w_A_hat_train.append(A_hat)\n",
    "            w_edit_A_hat_train.append(edit_A)\n",
    "            w_gen_A_hat_train.append(gen_A.detach())\n",
    "            gen_A_raw_train.append(gen_A_raw.detach())\n",
    "            masked_norm_A = masked_normalization(gen_A_raw.detach(), Param_train[i])\n",
    "            masked_norm_A_hats.append(masked_norm_A)\n",
    "            gen_A_max_train.append(gen_A_max.detach())\n",
    "            gen_A_min_train.append(gen_A_min.detach())\n",
    "\n",
    "print(\"====================== G(z + aw) v.s. edit(G(z))  results =============================\")\n",
    "gen_A_discretized = debugDiscretizer(w_A_hat_train, w_edit_A_hat_train, gen_A_raw_train, gen_A_max_train, gen_A_min_train, w_gen_A_hat_train, masked_norm_A_hats, discretize_method=\"hard_threshold\", printMatrix=False, abortPickle=True)\n",
    "\n",
    "\n",
    "#debugDecoder(w_edit_A_hat_train, [], w_gen_A_hat_train, [], discretize_method=\"hard_threshold\", printMatrix=True)\n",
    "# drawGraph(w_A_train, w_A_hat_train, w_edit_A_hat_train, w_gen_A_hat_train)\n",
    "# drawGraphSaveFigure(w_A_train, w_A_hat_train, w_edit_A_hat_train, w_gen_A_hat_train, clearImage=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Original topology (averaged) ---\n",
      " density: 0.2392992481202991 \n",
      " diameter: -1.0 \n",
      " clustering coefficient: 0.4116737698692536 \n",
      " edges: 45.466857142857144 \n",
      " avgerage degree 0.2392992481202991\n",
      "\n",
      "--- Edit topology (averaged) ---\n",
      " density: 0.34844210526315667 \n",
      " diameter: -0.9977142857142857 \n",
      " clustering coefficient: 0.4847906626640346 \n",
      " edges: 66.204 \n",
      " avgerage degree 0.3484421052631567\n",
      "\n",
      "--- Reconstructed topology (averaged) ---\n",
      " density: 0.35126616541353156 \n",
      " diameter: -1.0 \n",
      " clustering coefficient: 0.4980684135029308 \n",
      " edges: 66.74057142857143 \n",
      " avgerage degree 0.35126616541353156\n",
      "\n"
     ]
    }
   ],
   "source": [
    "density_ori, diameter_ori, cluster_coef_ori, edges_ori, avg_degree_ori = topological_measure(batched_A_hat_discretized) # G(z)\n",
    "density_ed, diameter_ed, cluster_coef_ed, edges_ed, avg_degree_ed = topological_measure(w_edit_A_hat_train) # edit(G(z), edit_alpha)\n",
    "density_hat, diameter_hat, cluster_coef_hat, edges_hat, avg_degree_hat = topological_measure([torch.from_numpy(gen_A_discretized)]) # G(z + alpaha * w)\n",
    "print(f\"--- Original topology (averaged) ---\\n density: {density_ori} \\n diameter: {diameter_ori} \"\n",
    "      f\"\\n clustering coefficient: {cluster_coef_ori} \\n edges: {edges_ori} \\n avgerage degree {avg_degree_ori}\\n\")\n",
    "\n",
    "print(f\"--- Edit topology (averaged) ---\\n density: {density_ed} \\n diameter: {diameter_ed} \"\n",
    "      f\"\\n clustering coefficient: {cluster_coef_ed} \\n edges: {edges_ed} \\n avgerage degree {avg_degree_ed}\\n\")\n",
    "\n",
    "print(f\"--- Reconstructed topology (averaged) ---\\n density: {density_hat} \\n diameter: {diameter_hat} \"\n",
    "      f\"\\n clustering coefficient: {cluster_coef_hat} \\n edges: {edges_hat} \\n avgerage degree {avg_degree_hat}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4582])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, alpha_edit = -0.2, 1\n",
    "alpha_gen = a_w2 * F.relu(a_w1 * alpha_edit + a_b1) + a_b2\n",
    "alpha_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8971199848858813"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log(0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
